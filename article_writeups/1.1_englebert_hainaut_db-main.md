# Reflection on [DB-MAIN: A next generation meta-CASE](https://www-sciencedirect-com.proxybz.lib.montana.edu/science/article/pii/S0306437999000071)

#### Englebert, V., & Hainaut, J.-L. (1999). DB-main: A next generation meta-case. Information Systems, 24(2), 99–112. https://doi.org/10.1016/s0306-4379(99)00007-1 

[(home)](https://beqpolk1.github.io/csci-592-spring2022/)

---

Unfortunately, I read this article on the DB-MAIN meta-CASE tool somewhat by mistake. It was my intention to have read “Strategy for Database Application Evolution: The DB-MAIN Approach”, by Jean-Marc Hick and Jean-Luc Hainaut. Both articles appeared in the citations section of a different paper, and I confused the two due to the similar name and having an author in common. The intended reading had more of an orientation toward databases, while this one tilted heavily toward Software Engineering, specifically CASE tools.

CASE stands for “computer aided software engineering”; the tool described by Englebert and Hainaut is a “meta-CASE”, a program used to design and build other CASE tools. According to the authors, some of the common functions provided by CASE environments include, “collecting, storing, and managing specifications” (where “specifications” are formal specifications of a system expressed through the environment’s modeling capability), “querying, visualizing, and exploring specifications”, “generating reports and code”, and “transforming specifications according to different abstraction levels and paradigms”. While there are a spectrum of different CASE tools and environments that provide differing levels of functionality, Englebert and Hainaut seemed primarily concerned with the sort of system that would allow a user to build a specification using a formal modeling language, and then use that specification to automatically generate corresponding computer code, or perhaps transform it from one formal model to another.

At the time of writing, most available CASE tools specialized in a narrow field or certain type of model. This was the motivation to attempt to construct a meta-CASE, so that software engineers could extend CASE environments to new problem domains, paradigms, or languages; the authors give an example of an individual using DB-MAIN to create a Sybase generator in a matter of days.

Englebert and Hainaut provided a rather thorough, though still high-level description of the architecture and usage of DB-MAIN. It included tools to model and customize the interface, functionality, and meta-model of a CASE environment, including two specifically developed languages, Grasyla (for interface customization) and Voyager2 (for functionality customization).

However, I was primarily interested in the architecture of DB-MAIN and the ability to customize meta-models. The architecture was composed of, “…two fully integrated layers: the basic and the meta layers,” each of which had its own repository. The basic layer essentially provided the functions of a normal CASE environment, where software specifications could be formally defined (the software in this case being another CASE tool). The meta layer mirrored the “structural parts” of the basic repository, so that the CASE engineer could, “…reuse this image in the dynamic part to use, to extend, or to specialize the meta-model built in the basic repository.”

Meta-models, in the context of a CASE tool, would refer to the concepts and entities specific to the domain of the tool and the models used; examples of meta-model elements might be “class” and “inheritance” for a tool designed to work with UML and the OO paradigm, or “entity”, “property”, and “foreign key” for a tool using ER models to design a database. For the purposes of DB-MAIN, the authors essentially had to make use of “meta-meta-models” (still referred to as “meta-models”) in order to create abstractions or generalizations that could be used across different CASE tools.

The authors generalized a meta-model as a meta-class, and supported inheritance relationships between meta-classes. Meta-classes also supported properties and functions. Meta-models (e.g. an ER model used to design a database) contained specific meta-classes in their ontologies, which was the way of defining a meta-model (which is, in turn another meta-class). This robust architecture allowed for expressive relationships that still preserved advantages such as automated code generation. Englebert and Hainaut also provided an example with mathematical notation describing how meta-models can be coordinated through the inheritance relationships facilitated by the DB-MAIN meta-model. It is shown how the meta-classes of “datastore” and “entity-type” may have enough semantic similarity in two meta-models such that they can be represented by one supertype, “data”, in the DB-MAIN meta-model; this eliminates the need to define an explicit relationship between the two meta-classes.

In general, it was very interesting to see how the authors handled the problem of bridging gaps between seemingly disparate models and paradigms to create a unified solution. The problem in front of them was not simple: how to create a system that can generate systems that can automatically generate software meeting specifications in areas as distinct as object-oriented programming and database security? The use of a 2-layered architecture seemed key to the solution; it would allow users to work in a less general, more concrete environment to create their specifications, but still relate all elements back to entities in a repository at the meta-level as well, which then allows for a natural transformation between meta-models for entities with similar semantics. It makes me wonder if such an approach could be extended to a multi-model or unified database, where each constituent database/data storage paradigm has its own specific data, but some central coordinating service knows *what types* of data and structures are contained in each database and therefore where to look for certain information or how to connect certain information together.

Still, I was disappointed to realize I read the software engineering DB-MAIN article, instead of the database focused one. It was difficult to immediately see the connections to the world of relational and NoSQL databases, or what ideas from the paper may be valuable when integrating heterogeneous data sources. There were some general themes that might be relevant, such as the two-layered architecture of the DB-MAIN meta-CASE. This reminded me a lot of the architecture described in the “Unified Data Modeling for Relational and NoSQL Databases” reading from last week, where higher-order entities and concepts had to be defined in order to unify relational and NoSQL paradigms. What is interesting is that that approach explicitly defined the higher-order level and proscribed how it related to different data models, while the DB-MAIN approach would leave such definitions and relationships more flexible and customizable. In this way, I believe the DB-MAIN approach would allow for a more natural extension to other database paradigms and more powerful transformations and synchronizations between databases. While this article did get me thinking about more robust ways in which to design and structure meta-models of existing models and systems for maintaining them, I can’t help but think I would have gathered more specific and relevant facts and techniques from the “Strategy for Database Application Evolution” paper.

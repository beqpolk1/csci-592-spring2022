[(home)](https://beqpolk1.github.io/csci-592-spring2022/)

## Project Results, Conclusion, and Reflection

The final project result is a small Java application, MultiQuery, which demonstrates a proof of the concept of being able to retrieve query plans and cost metrics for the “same” query on different database systems. For the purposes of demonstration, it uses two sets of test queries (where a “set” of queries refers to a trio of MongoDB, PostgreSQL, and Neo4j queries that all return the same data); one set demonstrates functionality for a more complicated retrieval, and one set is a much simpler retrieval for the purposes of showing how the application of indices at the individual database level affects results.

MultiQuery connects to Mongo, Postgres, and Neo4j instances running in Docker containers, populated with testing data as described in the [document on initializing the test bed](./updates/test_env.html). Within each query set there are multiple components: the “base query”, which is the query that would actually scan the database and return results, the “count query”, which returns the count of results obtained by the base query, and the “plan query”, which issues a command to the database to return an execution plan of the base query. The document “/project/uniform_queries.txt” contains these queries for each database for the more complicated retrieval statement (query set 1). The simpler retrieval statement (query set 2) is not located outside the MultiQuery source code, as it is a much simpler select and filter operation. For each set, MultiQuery issues two queries to each database: the count query and the plan query (the base query is never actually executed). The use of count queries provides some assurances that the queries between different databases are in fact returning the same data, while the output of the plan queries is further parsed into another data structure.

Again, for demonstration purposes, the output of plan queries is parsed into a JSON object (using the Google GSON library) in order to provide flexibility for the results, since each database has markedly different ways of structuring plans. Mongo plan results are first processed from the native return type into a JSON object, and then processed further. Neo4j plan results are returned in a Plan object that is native to the Neo4j driver for Java, and this Plan object is then processed further. Finally, Postgres plan results are returned as a single XML string from the database; this XML is built into an XML document in Java and then processed further.

The MultiQuery application demonstrates the concept of having the “same” query for multiple databases (which contain the “same” data) and being able to programmatically analyze the proposed execution plan and estimated cost of running the query in each database. It shows that, in general, this is possible using mature databases with very distinct paradigms (document, graph, and relational) that offer bindings and support for other programming languages in which to build a “meta-analysis” layer. It also shows how an application could perhaps detect that indices are applied in certain databases but not in others, and thus make informed decisions about where to retrieve data. As noted, this is best demonstrated by the simpler data retrieval done in query set 2. There are statements in the “/project/uniform_queries.txt” document for creating and deleting the relevant indices in Mongo and Neo4j; the scripts for index creation and deletion in Postgres are contained in their own files in the “/project/postgresScripts” directory.

As noted in the proposal document, MultiQuery would fit into the “Evaluator” component of some generic query executer. No attempt was made to automatically translate from one query language to another; all queries were hard-coded within the application. Work on this application has revealed that the notion of query translation would be a very difficult problem from an implementation point of view (regardless of the many theoretical difficulties sure to be encountered as well). This is partly due to the nature of the bindings and driver offered by Mongo for Java. While Postgres and Neo4j both offer faculties to essentially define a query as a string, execute that string against the database, and obtain results, the Mongo driver is much more oriented toward defining a query in terms of objects within the host application. I am unsure if this is specific to Java, or a shared trait that applies to other languages with which Mongo integrates. For this project, this approach proved to be a significant roadblock against obtaining query plans for “dynamically” defined queries, and would likely continue to cause problems for any query translation attempts.

Further limitations relate to the information available from the query planner for each database. Perhaps unsurprisingly, Postgres offers the most comprehensive breakdown and analysis of a query execution plan. I expected this given relational databases’ long history, rich theoretical and mathematical underpinnings, and the amount of development and refinement that has gone into optimizing plans for relational queries. The thorough information provided by Postgres would surely be enough for an analysis engine to make an educated guess as to whether it was the proper environment for a query.

Mongo and Neo4j, however, returned a relative lack of planning information that would make it difficult to assess comparative costs of a query programmatically. For Mongo, part of the issue is that it does substantial processing in memory, which can hide what is actually going on with a query. For the complex retrieval operation (query set 1), the query planning seems to “stop” with the first retrieval of the elements of the collection on which to apply the aggregation pipeline. It properly indicates that it will perform a full collection scan, the number of elements and “work” units that will involve, and estimated time numbers. But for later aggregation steps, including significant operations such as a “$lookup” in another collection, there is no additional information. In this way, it is hard to believe that the query plan is providing a complete picture. One interesting thing to note, however, is that the returned execution plan does reflect a reordering (and even on removal) of the aggregation operations in the pipeline, presumably because I did not order them in the most efficient way.

Neo4j, on the other hand, seems to only include cardinality information in its query plan considerations. As my [research on testing methods](testing_methods.md) indicated, this is often a paramount concern when evaluating execution plans. Yet the lack of any other cost, memory, or time indicators is troubling, and it is difficult to see how analysis of a Neo4j query plan would reveal that it is likely the best choice for connectivity or relationship-oriented queries (especially those that connect multiple instances of the same entity type together). Neo4j does offer a more powerful “PROFILE” option (as exposed to “EXPLAIN”), which provides some additional information, but there are two drawbacks to this. One is that PROFILE *actually executes* the candidate query, instead of merely planning it, thus incurring significant extra cost. Second, when using PROFILE, the results of the explain query did not include any planning information when returned to Java. It is unclear if this is by design, a bug in the Neo4j Java driver, or user error on my part. Yet I could not find any way to get the more detailed PROFILE results returned to the MultiQuery application.

Even with these limitations, a large amount of potential work still exists that could refine this application and turn it into something useful. The concerns listed above are primarily practical rather than theoretical in nature; it is possible to hook the pieces together, and future improvements to libraries, drivers, or query plan information could readily be incorporated into the application. The biggest, and probably even more important task, lies in discovering the key attributes of each type of query plan and how they relate to real-world performance. For the demonstration, a few crude assumptions were made on the statistics of interest:
* For Mongo plans, MultiQuery searches for “executionStats” objects in the results and tracks the sums of the values of the following fields for all such objects: “nReturned”, “totalDocsExamined”, “executionStages.executionTimeMillisEstimate”, “executionStages.works”, and “executionStages.needTime”. It is designed in a way that should accommodate multi-stage queries, although neither of the query sets exercise this functionality (see above comment on the query planner “hiding” steps of an aggregation pipeline).
* For Neo4j plans, MultiQuery sums the “EstimatedRows” field from the ```arguments``` object of the plan and all child plans (where the ```arguments``` object contains such a field). It uses a breadth-first search to iterate through all child plans of the query plan.
* For Postgres plans, MultiQuery finds the maximum values of the following fields for any ```Plan``` node in the XML document: “Startup-Cost”, “Total-Cost”, “Actual-Rows”, “Actual-Startup-Time”, and “Actual-Total-Time”. It uses a breadth-first search to iterate through all ```Plan``` nodes in the XML document.

Surely, there are more important statistics to consider and better ways of aggregating statistics from plans that contain multiple components. As mentioned above, an important and likely non-trivial task would be to determine which statistics are directly relevant to the observed performance of each database. This would be greatly aided by the fact that the test bed for this application is portable, and that it is designed to work with synthetically generated datasets of varying sizes. Implementing data converters and designing query test sets for other schemas besides the modified MongoSongs would still be labor intensive. Yet it should be possible to come up with robust query sets that exercise many different scenarios and run those sets on databases of many different sizes, on many different machines. The resulting trove of data could be compiled and then analyzed using data-mining or regression techniques to determine which aspects of a query plan have the highest relation to observed performance.

Also worth noting is that the raw results returned by each plan query do seem to indicate when indices are available and leveraged in the underlying database. I did not build MultiQuery to specifically look for and catalog the presence or use of such indices in query plans, so that would be another step toward developing a wholistic outlook on which database would likely provide the best performance for a query.

My experiences developing MultiQuery have also led to some observations on how such a tool, and even a more comprehensive “Generic U-Schema Client Tool”, might be useful. To be frank, I believe that the amount of overhead from middleware processing would rule out use for high-throughput, online applications. Even with hard-coded queries (i.e. no translation), there is a significant amount of baggage involved with connecting to, querying against, and processing results from different databases in a “meta-layer” application. It requires a significant amount of infrastructure to communicate with different databases and process results into a common format that can power further analysis. I believe it would be very difficult to develop a solution that scaled well in relation to request volume.

That being said, I think this sort of tooling *would* still be quite valuable for large organizations that are already using a polyglot persistence approach, especially if the teams working with each individual storage solution are somewhat siloed from each other. Suppose that an organization wanted to develop a data warehouse incorporating data from multiple databases, each of which uses a different paradigm and is fed from a fairly independent product, and the product owners are largely independent teams who are able to develop without grossly affecting one another. In such a situation, a tool that could look for certain data across storage solutions, offer a unified storage model, and provide analysis to support which data should be extracted from which system could be quite useful. The designers of the data warehouse (and associated data pipelines) could leverage a tool that provided more “offline” capabilities, performing single exploratory operations and queries whose results would inform the overall data engineering process. The true power of a U-Schema-based, data-paradigm-agnostic tool would be in unifying already existing data sources, as opposed to building a novel database from the ground up.

To close, I’d like to spend a little time reflecting on the outcome of the project and how it aligns with the project proposal and the goals of the independent study. The most immediate observation is that the objectives of the project proposal were far too ambitious. I significantly underestimated the amount of time required for various parts of the implementation; working with three database systems meant that many problems were multiplied three-fold. Since Mongo operates under such a different querying framework, it proved to be a particularly problematic. While I don’t regret using the synthetic data generator from the ModelUM group and running everything through Docker, these two aspects added a significant amount of time to the project due to troubleshooting and familiarizing myself with new technologies. Still, I believe that was those were the right choices for an application that could provide significant further research utility.

Overall, the final project went in a somewhat different direction than the original proposal and the scope laid out for the independent study. I did not develop a meta-dictionary, or rudimentary U-Schema cost-estimation mechanism. I would not necessarily say that I explored multiple techniques for coping with issues in a polyglot persistence environment; most of the semester was focused on data modeling approaches.

All that being said, I’m still happy with the work that I was able to complete and what I learned along the way. I do think that I gained a lot of experience with what it’s like to work in a polyglot persistence system, and how difficult it can be to develop middleware for that environment. I definitely learned a lot more about the modeling concerns underlying major database paradigms, as well as how to structure data to fit each paradigm and how each one processes queries. I became much more familiar with several valuable tools, such as Docker and Eclipse. While the bulk of the knowledge I gained during this course was “experiential” as opposed to “theoretical”, and the outcome strayed somewhat from the original intentions, it was a valuable experience and I’m very grateful to have had the opportunity to pursue this line of study.
